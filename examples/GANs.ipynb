{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code source: Sebastian Curi and Andreas Krause, based on Jaques Grobler (sklearn demos).\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# We start importing some modules and running some magic commands\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# General math and plotting modules.\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import erfinv\n",
    "from scipy import linalg\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "\n",
    "# Project files.\n",
    "from utilities.util import gradient_descent\n",
    "from utilities.classifiers import Logistic\n",
    "from utilities.regularizers import L2Regularizer\n",
    "from utilities.load_data import polynomial_data, linear_separable_data\n",
    "from utilities import plot_helpers\n",
    "from utilities.widgets import noise_widget, n_components_widget, min_prob_widget\n",
    "\n",
    "# Widget and formatting modules\n",
    "import IPython\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual, fixed\n",
    "from matplotlib import rcParams\n",
    "import matplotlib as mpl \n",
    "\n",
    "# If in your browser the figures are not nicely vizualized, change the following line. \n",
    "rcParams['figure.figsize'] = (10, 5)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "# Machine Learning library. \n",
    "import torch \n",
    "import torch.jit\n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, weights, means, scales):\n",
    "        self.num_centers = len(weights)\n",
    "        self.weights = weights / np.sum(weights)\n",
    "        self.means = means\n",
    "        self.scales = scales \n",
    "    \n",
    "    def sample(self, batch_size=1):\n",
    "        centers = np.random.choice(self.num_centers, batch_size, p=self.weights)\n",
    "        eps = np.random.randn(batch_size)\n",
    "        return self.means[centers] + eps * self.scales[centers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gmm(true_model, sampling_model,  title):\n",
    "    gaussians = [norm(mean, scale) for mean, scale in zip(true_model.means, true_model.scales)]\n",
    "    scale = sum(true_model.weights)\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    X = np.linspace(-1.25, 1.25, 1000)\n",
    "    y = np.zeros_like(X)\n",
    "    for i, (weight, gaussian) in enumerate(zip(true_model.weights, gaussians)):\n",
    "        y += weight * gaussian.pdf(X) / scale\n",
    "\n",
    "    ax.plot(X, y, label='Exact PDF')\n",
    "    try:\n",
    "        ax.hist(sampling_model.sample(10000), bins=100, density=True, label='Samples')\n",
    "    except ValueError:\n",
    "        ax.hist(sampling_model.sample(10000)[0][:, 0], bins=100, density=True, label='Samples')\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_xlim([-1.25, 1.25])\n",
    "    ax.set_title(title)\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    IPython.display.display(fig)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Given a random input, produce a random output.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int, noise='uniform'):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.noise = noise\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 15),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(15, output_dim),\n",
    "            nn.Tanh()  # Distribution is bounded between -1 and 1.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "    def rsample(self, batch_size=1):\n",
    "        \"\"\"Get a differentiable sample of the generator model.\"\"\"\n",
    "        if self.noise == 'uniform':\n",
    "            noise = torch.rand(batch_size, self.input_dim)\n",
    "\n",
    "        elif self.noise == 'normal':\n",
    "            noise = torch.randn(batch_size, self.input_dim)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        return self(noise).squeeze(-1)\n",
    "\n",
    "    def sample(self, batch_size=1):\n",
    "        \"\"\"Get a sample of the generator model.\"\"\"\n",
    "        return self.rsample(batch_size).detach()\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminate if true from fake samples.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, 25),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(25, 1),\n",
    "            nn.Sigmoid()  # Output is bounded between 0 and 1.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, true_model, generator_optimizer, discriminator_optimizer, \n",
    "              num_iter, discriminator_loss, generator_loss, plot_freq=1000, batch_size=64):\n",
    "    loss = nn.BCELoss()\n",
    "    for i in range(num_iter):\n",
    "        true_data = torch.tensor(true_model.sample(batch_size)).float().unsqueeze(-1)\n",
    "        fake_data = generator.rsample(batch_size).unsqueeze(-1)\n",
    "        # equivalently, fake_data = generator(torch.randn(batch_size, code_size)).squeeze()\n",
    "\n",
    "        true_label = torch.full((batch_size,), 1.)\n",
    "        fake_label = torch.full((batch_size,), 0.)\n",
    "\n",
    "        ###################################################################################\n",
    "        # Update G network: maximize log(D(G(z)))                                         #\n",
    "        ###################################################################################\n",
    "        generator_optimizer.zero_grad()\n",
    "        loss_g = loss(discriminator(fake_data), true_label)  # true label.\n",
    "        loss_g.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        generator_loss.append(loss_g.item())\n",
    "\n",
    "        ###################################################################################\n",
    "        # Update D network: maximize log(D(x)) + log(1 - D(G(z)))                         #\n",
    "        ###################################################################################\n",
    "        discriminator_optimizer.zero_grad()\n",
    "\n",
    "        # train on true data.\n",
    "        loss_d_true = loss(discriminator(true_data), true_label)\n",
    "        loss_d_true.backward()\n",
    "\n",
    "        # train on fake data.\n",
    "        loss_d_fake = loss(discriminator(fake_data.detach()), fake_label)\n",
    "        loss_d_fake.backward()\n",
    "\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        loss_d = loss_d_true + loss_d_fake\n",
    "        discriminator_loss.append(loss_d.item())\n",
    "\n",
    "        if plot_freq and i % plot_freq == 0:\n",
    "            ax = plot_gmm(true_model, generator, f\"Episode {i}\")\n",
    "    \n",
    "    return discriminator_loss, generator_loss\n",
    "\n",
    "\n",
    "def train_gan_interactive(num_iter, true_model, noise_model, noise_dim, generator_lr, discriminator_lr):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    generator = Generator(input_dim=noise_dim, output_dim=1, noise=noise_model)\n",
    "    discriminator = Discriminator(input_dim=1)\n",
    "    \n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=generator_lr, betas=(0.5, 0.999))\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=discriminator_lr, betas=(0.5, 0.99))\n",
    "\n",
    "    discriminator_loss, generator_loss = [], []\n",
    "    try:\n",
    "        train_gan(generator, discriminator, true_model, generator_optimizer, discriminator_optimizer, num_iter, discriminator_loss, generator_loss)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    plot_gmm(true_model, generator, \"Final Generator Model\")\n",
    "    plt.plot(generator_loss, label='Generator Loss')\n",
    "    plt.plot(discriminator_loss, label='Discriminator Loss')\n",
    "    plt.xlabel('Iteration Number')\n",
    "    plt.ylabel(' Loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN's for fitting a Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20, 8)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "gaussian_model = GMM(weights=np.array([1.]),means=np.array([0.5]), scales=np.array([0.2])) \n",
    "plot_gmm(gaussian_model, gaussian_model, 'Exact Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20, 8)\n",
    "rcParams['font.size'] = 16\n",
    "num_iter = 15000\n",
    "interact_manual(lambda noise_model, noise_dim, generator_lr, discriminator_lr: train_gan_interactive(\n",
    "    num_iter, gaussian_model, noise_model, noise_dim, generator_lr, discriminator_lr),\n",
    "                noise_model=ipywidgets.Dropdown(options=['uniform', 'normal'], value='normal', description='Noise model:', style={'description_width': 'initial'}, continuous_update=False),\n",
    "                noise_dim=ipywidgets.IntSlider(min=1, max=10, value=4, description='Noise dimension:', style={'description_width': 'initial'}, continuous_update=False),\n",
    "                generator_lr=ipywidgets.FloatLogSlider(value=1e-4, min=-6, max=0, description=\"Generator lr\", style={'description_width': 'initial'}, continuous_update=False),\n",
    "                discriminator_lr=ipywidgets.FloatLogSlider(value=1e-4, min=-6, max=0, description=\"Discriminator lr\", style={'description_width': 'initial'}, continuous_update=False),\n",
    "               );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN's for fitting a GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20, 8)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "gmm_model = GMM(weights=np.array([0.3, 0.5, 0.2]),\n",
    "                 means=np.array([-3., 0., 2.]) / 5,\n",
    "                 scales=np.array([0.5, 1.0, 0.1]) / 5)\n",
    "plot_gmm(gmm_model, gmm_model, 'Exact Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20, 8)\n",
    "rcParams['font.size'] = 16\n",
    "num_iter = 15000\n",
    "interact_manual(lambda noise_model, noise_dim, generator_lr, discriminator_lr: train_gan_interactive(\n",
    "    num_iter, gmm_model, noise_model, noise_dim, generator_lr, discriminator_lr),\n",
    "                noise_model=ipywidgets.Dropdown(options=['uniform', 'normal'], value='normal', description='Noise model:', style={'description_width': 'initial'}, continuous_update=False),\n",
    "                noise_dim=ipywidgets.IntSlider(min=1, max=10, value=8, description='Noise dimension:', style={'description_width': 'initial'}, continuous_update=False),\n",
    "                generator_lr=ipywidgets.FloatLogSlider(value=1e-4, min=-6, max=0, description=\"Generator lr\", style={'description_width': 'initial'}, continuous_update=False),\n",
    "                discriminator_lr=ipywidgets.FloatLogSlider(value=1e-3, min=-6, max=0, description=\"Discriminator lr\", style={'description_width': 'initial'}, continuous_update=False),\n",
    "               );\n",
    "# Generator lr <= 1e-5 shows mode collapse\n",
    "# Generator lr >= 1e-3 shows oscillation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "gmm_model = GMM(weights=np.array([0.3, 0.5, 0.2]),\n",
    "                 means=np.array([-3., 0., 2.]) / 5,\n",
    "                 scales=np.array([0.5, 1.0, 0.1]) / 5)\n",
    "\n",
    "X = gmm_model.sample(1000)\n",
    "def interact_gmm_fit(n_components):\n",
    "    gmm = GaussianMixture(n_components=n_components).fit(X[:, np.newaxis])\n",
    "    plot_gmm(gmm_model, gmm, 'Fitted GMM')\n",
    "    \n",
    "interact(interact_gmm_fit, n_components=ipywidgets.IntSlider(value=3, min=1, max=10, description=\"Num Components\", style={'description_width': 'initial'}, continuous_update=False)\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST DCGANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels in the training images. For mnist images this is 3\n",
    "nc = 1\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator, self).__init__() \n",
    "        self.input_dim = input_dim\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(input_dim, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return self.main(x)\n",
    "    \n",
    "    def rsample(self, batch_size: int = 1):\n",
    "        \"\"\"Get a differentiable sample of the generator model.\"\"\"\n",
    "        noise = torch.randn(batch_size, self.input_dim, 1, 1).to(device=device)\n",
    "        return self(noise)\n",
    "    \n",
    "    def sample(self, batch_size: int=1):\n",
    "        \"\"\"Get a sample of the generator model.\"\"\"\n",
    "        return self.rsample(batch_size).detach()\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.main(x).view(-1)\n",
    "    \n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "def plot_mnist_gan(fake_data, true_data, epoch, n_epoch):\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        plt.close()\n",
    "\n",
    "        fig = plt.figure(constrained_layout=True)\n",
    "        gs = fig.add_gridspec(2, 2)\n",
    "        ax1 = fig.add_subplot(gs[0, 0])  # Fake Data\n",
    "        ax2 = fig.add_subplot(gs[0, 1])  # Real Data\n",
    "        ax3 = fig.add_subplot(gs[1, :])  # Monitor losses\n",
    "\n",
    "        w = fake_data[:50].detach().cpu()\n",
    "        grid = torchvision.utils.make_grid(w, nrow=10, padding=5)\n",
    "        ax1.imshow(np.transpose(grid.numpy(), (1,2,0)), interpolation='nearest')\n",
    "        if epoch == n_epoch:\n",
    "            ax1.set_title(f\"Fake Data Final.\")\n",
    "        else:\n",
    "            ax1.set_title(f\"Fake Data Epoch: {epoch}/{n_epoch}.\")\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "\n",
    "        w = true_data[:50].detach().cpu()\n",
    "        grid = torchvision.utils.make_grid(w, nrow=10, padding=5)\n",
    "        ax2.imshow(np.transpose(grid.numpy(), (1,2,0)), interpolation='nearest')\n",
    "        if epoch == n_epoch:\n",
    "            ax2.set_title(f\"True Data Final.\")\n",
    "        else:\n",
    "            ax2.set_title(f\"True Data Epoch: {epoch}/{n_epoch}.\")\n",
    "\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_yticks([])\n",
    "\n",
    "        ax3.plot(generator_loss, label='Generator Loss')\n",
    "        ax3.plot(discriminator_loss, label='Discriminator Loss')\n",
    "        ax3.set_xlabel('Iteration Number')\n",
    "        ax3.set_ylabel('Loss')\n",
    "        ax3.legend(loc='best')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if epoch == n_epoch:\n",
    "            plt.savefig('final.png')\n",
    "        else:\n",
    "            plt.savefig(f'epoch{epoch}.png')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we do not have a generative model, but only data samples. \n",
    "rcParams['figure.figsize'] = (16, 10)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "z_dim = 100\n",
    "generator_lr = 0.0002\n",
    "discriminator_lr = 0.0005 \n",
    "beta1 = 0.5\n",
    "\n",
    "batch_size = 128\n",
    "dataset_size = 60000\n",
    "t2pil = transforms.ToPILImage()\n",
    "\n",
    "img_size = 64\n",
    "transform = transforms.Compose([\n",
    "        transforms.Scale(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "generator = Generator(input_dim=z_dim).to(device)\n",
    "generator = torch.jit.script(generator)\n",
    "\n",
    "generator.apply(weights_init)\n",
    "discriminator = Discriminator(img_size).to(device)\n",
    "discriminator = torch.jit.script(discriminator)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=generator_lr, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=discriminator_lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "n_epoch = 20\n",
    "discriminator_loss, generator_loss = [], []\n",
    "try:\n",
    "    for epoch in range(n_epoch):           \n",
    "        for batch_idx, (x, _) in enumerate(train_loader):\n",
    "            true_data = x.to(device)  # only this line changes.\n",
    "            noise = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "            fake_data = generator(noise)\n",
    "\n",
    "            true_label = torch.full((batch_size,), 1.).to(device)\n",
    "            fake_label = torch.full((batch_size,), 0.).to(device)\n",
    "\n",
    "            ###################################################################################\n",
    "            # Update G network: maximize log(D(G(z)))                                         #\n",
    "            ###################################################################################\n",
    "            generator_optimizer.zero_grad()\n",
    "            loss_g = loss(discriminator(fake_data), true_label)  # true label.\n",
    "            loss_g.backward()\n",
    "            generator_optimizer.step()\n",
    "\n",
    "            generator_loss.append(loss_g.item())\n",
    "\n",
    "            ###################################################################################\n",
    "            # Update D network: maximize log(D(x)) + log(1 - D(G(z)))                         #\n",
    "            ###################################################################################\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            # train on true data.\n",
    "            loss_d_true = loss(discriminator(true_data), true_label)\n",
    "            loss_d_true.backward()\n",
    "\n",
    "            # train on fake data.\n",
    "            loss_d_fake = loss(discriminator(fake_data.detach()), fake_label)\n",
    "            loss_d_fake.backward()\n",
    "\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            loss_d = loss_d_true + loss_d_fake\n",
    "            discriminator_loss.append(loss_d.item())\n",
    "            \n",
    "            if batch_idx == 0:\n",
    "                plot_mnist_gan(fake_data, true_data, epoch, n_epoch, batch_idx, len(train_loader))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "plot_mnist_gan(fake_data, true_data, n_epoch, n_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

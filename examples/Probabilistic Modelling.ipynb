{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code source: Sebastian Curi and Andreas Krause, based on Jaques Grobler (sklearn demos).\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# We start importing some modules and running some magic commands\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# General math and plotting modules.\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import erfinv\n",
    "\n",
    "# Project files.\n",
    "from utilities.util import gradient_descent\n",
    "from utilities.classifiers import Logistic\n",
    "from utilities.regressors import TStudent\n",
    "from utilities.regularizers import L2Regularizer\n",
    "from utilities.load_data import polynomial_data, linear_separable_data\n",
    "from utilities import plot_helpers\n",
    "\n",
    "# Widget and formatting modules\n",
    "import IPython\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual, fixed\n",
    "from matplotlib import rcParams\n",
    "# If in your browser the figures are not nicely vizualized, change the following line. \n",
    "rcParams['figure.figsize'] = (10, 5)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "# Machine Learning library. \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDRegressor, Ridge, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_dataset(dataset, X=None, n_samples=200, noise=0, w=None):\n",
    "    if X is None:\n",
    "        X = np.random.randn(n_samples)\n",
    "    \n",
    "    if dataset == 'cos':\n",
    "        Y = np.cos(1.5 * np.pi * X) + noise * np.random.randn(X.shape[0])\n",
    "        \n",
    "    elif dataset == 'sinc':\n",
    "        Y = X * np.sin(1.5 * np.pi * X) + noise * np.random.randn(X.shape[0])\n",
    "        \n",
    "    elif dataset == 'linear':\n",
    "        X = np.atleast_2d(X).T\n",
    "        Phi = PolynomialFeatures(degree=1, include_bias=True).fit_transform(X)\n",
    "        Y = Phi @ w[:2] + noise * np.random.randn(X.shape[0])\n",
    "    \n",
    "    elif dataset == 'linear-features':\n",
    "        X = np.atleast_2d(X).T\n",
    "        Phi = PolynomialFeatures(degree=len(w) - 1, include_bias=True).fit_transform(X)\n",
    "        Y = Phi @ w + noise * np.random.randn(X.shape[0])\n",
    "    \n",
    "    return X, Y\n",
    "    \n",
    "\n",
    "def get_classification_dataset(dataset, n_samples=200, noise=0.3):\n",
    "    if dataset == 'linear':\n",
    "        X, Y = linear_separable_data(n_samples, noise=noise, dim=2) \n",
    "        Y = (Y + 1) // 2\n",
    "    elif dataset == '2-blobs':\n",
    "        X, Y = datasets.make_classification(n_classes=2, n_features=2, n_informative=2, n_redundant=0,\n",
    "                                            n_clusters_per_class=1, n_samples=n_samples, random_state=8)\n",
    "    elif dataset == '3-blobs':\n",
    "        X, Y = datasets.make_classification(n_classes=3, n_features=2, n_informative=2, n_redundant=0,\n",
    "                                            n_clusters_per_class=1, n_samples=n_samples, random_state=8)\n",
    "    elif dataset == '4-blobs':\n",
    "        X, Y = datasets.make_classification(n_classes=4, n_features=2, n_informative=2, n_redundant=0,\n",
    "                                            n_clusters_per_class=1, n_samples=n_samples, random_state=8) \n",
    "    elif dataset == 'circles':\n",
    "        X, Y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "    elif dataset == 'moons':\n",
    "        X, Y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "    elif dataset == 'iris':\n",
    "        X, Y = datasets.load_iris(return_X_y=True)\n",
    "        X = X[:, :2]\n",
    "    elif dataset == 'imbalanced':\n",
    "        X, Y = linear_separable_data(n_samples, noise=noise, dim=2, num_negative=int(n_samples * 0.2))\n",
    "        Y = (Y + 1) // 2\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Regression\n",
    "\n",
    "We compare a regressor that uses a gaussian likelihood vs. one that uses a student-t likelihood with 2 degrees of freedom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "def probabilistic_regression(dataset, nu, n_samples, degree, alpha, noise, noise_type):\n",
    "    np.random.seed(0)\n",
    "    # DATASET\n",
    "\n",
    "    w = np.random.randn(1 + degree)\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    _, y = get_regression_dataset(dataset, X=X, noise=0, w=w)\n",
    "    ymean = np.mean(y)\n",
    "    if noise_type == 'gaussian':\n",
    "        y += noise * np.random.randn(*y.shape)\n",
    "    elif noise_type == 'heavy-tailed':\n",
    "        y += noise * np.random.standard_cauchy(*y.shape)\n",
    "    y = y - np.mean(y)\n",
    "\n",
    "    # REGRESSION\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    Phi = polynomial_features.fit_transform(X[:, np.newaxis])\n",
    "    Phimean = Phi.mean(axis=0)\n",
    "    \n",
    "    normal = Ridge(alpha=alpha)\n",
    "    normal.fit(Phi - Phimean, y)\n",
    "    \n",
    "    student = TStudent(x=Phi - Phimean, y=y, nu=nu, sigma=noise)\n",
    "    regularizer = L2Regularizer(alpha, include_bias=False)\n",
    "    opts = {'eta0': 0.1, 'n_iter': 1000, 'batch_size': min(n_samples, 64), 'n_samples': X.shape[0], \n",
    "            'algorithm': 'SGD'}\n",
    "    gradient_descent(normal.coef_, student, regularizer, opts=opts)\n",
    "    \n",
    "    # PREDICT    \n",
    "    X_plot = np.linspace(-1, 2, 100)\n",
    "    Phi_plot = polynomial_features.fit_transform(X_plot[:, np.newaxis]) - Phimean\n",
    "    _, Y_plot = get_regression_dataset(dataset, X=X_plot, noise=0, w=w)\n",
    "    Y_plot -= ymean\n",
    "    \n",
    "    # PLOTS\n",
    "    plt.plot(X_plot, student.predict(Phi_plot), 'g-', label=\"Student\")\n",
    "    plt.plot(X_plot, normal.predict(Phi_plot), 'r-', label=\"Normal\")\n",
    "    plt.plot(X_plot, Y_plot, 'b--', label=\"True function\")\n",
    "\n",
    "    plt.scatter(X, y, edgecolor='b', s=20)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((-0.5, 1.5))\n",
    "    plt.ylim((-1 + np.min(Y_plot), 1 + np.max(Y_plot)))\n",
    "    plt.legend(loc=\"upper left\", ncol=4)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "interact(probabilistic_regression,  dataset=['cos', 'sinc', 'linear', 'linear-features'], \n",
    "         nu=ipywidgets.FloatLogSlider(value=1, min=-2, max=4, step=0.01, readout_format='.4f',\n",
    "                                      description='Nu:', continuous_update=False),\n",
    "         \n",
    "         n_samples=ipywidgets.IntSlider(value=300, min=30, max=1500, step=1, \n",
    "                                        description='Samples:', continuous_update=False),\n",
    "         degree=ipywidgets.IntSlider(value=1, min=1, max=15, step=1, \n",
    "                                     description='Degree:', continuous_update=False),\n",
    "         noise=ipywidgets.FloatSlider(value=0.1, min=0, max=1, step=0.01, readout_format='.2f',\n",
    "                                      description='Noise level:', continuous_update=False),\n",
    "         alpha=ipywidgets.BoundedFloatText(value=0, min=0, max=1000, step=0.0001, \n",
    "                                           description='Reg Coef.:', continuous_update=False),\n",
    "         noise_type=['gaussian', 'heavy-tailed']\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "# Probabilistic Classification (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20, 6)\n",
    "rcParams['font.size'] = 22\n",
    "\n",
    "num_points_w = ipywidgets.IntSlider(value=300, min=30, max=1500, step=1, description='Number of samples:',\n",
    "                                   style={'description_width': 'initial'}, continuous_update=False)\n",
    "noise_w = ipywidgets.FloatSlider(value=0.1, min=0, max=1, step=0.01, readout_format='.2f', description='Noise level:',\n",
    "                                 style={'description_width': 'initial'}, continuous_update=False)\n",
    "reg_w = ipywidgets.BoundedFloatText(value=0, min=0, max=1000, step=0.0001, description='Regularization:',\n",
    "                                    style={'description_width': 'initial'}, continuous_update=False)\n",
    "batch_size_w = ipywidgets.IntSlider(value=16, min=1, max=64, step=1, description='Batch Size:',\n",
    "                                   style={'description_width': 'initial'}, continuous_update=False)\n",
    "lr_w = ipywidgets.FloatLogSlider(value=0.3, min=-4, max=1, step=0.1, readout_format='.4f', description='Learning Rate:',\n",
    "                                 style={'description_width': 'initial'}, continuous_update=False)\n",
    "num_iter_w = ipywidgets.IntSlider(value=50, min=10, max=200, step=1, description='Num Iter:',\n",
    "                                   style={'description_width': 'initial'}, continuous_update=False)\n",
    "def logistic_SGD(dataset, num_points, noise, reg, batch_size, lr, num_iter):\n",
    "#     np.random.seed(42)\n",
    "    \n",
    "    # DATASET\n",
    "    X, Y = get_classification_dataset(dataset, num_points, noise)\n",
    "    Y = 2 * Y - 1 \n",
    "    if X.shape[1] == 2:\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X, ones), axis=-1)\n",
    "    \n",
    "    Xtest, Ytest = get_classification_dataset(dataset, int(0.1 * num_points), noise)\n",
    "    Ytest = 2 * Ytest - 1 \n",
    "    if Xtest.shape[1] == 2:\n",
    "        ones = np.ones((Xtest.shape[0], 1))\n",
    "        Xtest = np.concatenate((Xtest, ones), axis=-1)\n",
    "\n",
    "    indexes = np.arange(0, X.shape[0], 1)\n",
    "    np.random.shuffle(indexes)\n",
    "    X, Y = X[indexes], Y[indexes]\n",
    "\n",
    "    # REGRESSION\n",
    "    classifier = Logistic(X, Y)\n",
    "    classifier.load_test_data(Xtest, Ytest)\n",
    "    regularizer = L2Regularizer(reg)\n",
    "    np.random.seed(42)\n",
    "    w0 = np.random.randn(3, )\n",
    "    \n",
    "    opts = {'eta0': lr,\n",
    "            'n_iter': num_iter,\n",
    "            'batch_size': min(batch_size, X.shape[0]),\n",
    "            'n_samples': X.shape[0],\n",
    "            'algorithm': 'SGD',\n",
    "            }\n",
    "    \n",
    "    try:\n",
    "        trajectory, indexes = gradient_descent(w0, classifier, regularizer, opts)\n",
    "        \n",
    "        # PLOTS\n",
    "        contour_plot = plt.subplot(121)\n",
    "        error_plot = plt.subplot(122)\n",
    "\n",
    "        opt = {'marker': 'ro', 'fillstyle': 'full', 'label': '+ Train', 'size': 8}\n",
    "        plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=contour_plot, options=opt)\n",
    "        opt = {'marker': 'bs', 'fillstyle': 'full', 'label': '- Train', 'size': 8}\n",
    "        plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=contour_plot, options=opt)\n",
    "\n",
    "        opt = {'marker': 'ro', 'fillstyle': 'none', 'label': '+ Test', 'size': 8}\n",
    "        plot_helpers.plot_data(Xtest[np.where(Ytest == 1)[0], 0], Xtest[np.where(Ytest == 1)[0], 1], fig=contour_plot, options=opt)\n",
    "        opt = {'marker': 'bs', 'fillstyle': 'none', 'label': '- Test', 'size': 8}\n",
    "        plot_helpers.plot_data(Xtest[np.where(Ytest == -1)[0], 0], Xtest[np.where(Ytest == -1)[0], 1], fig=contour_plot, options=opt)\n",
    "\n",
    "        contour_opts = {'n_points': 100, 'x_label': '$x$', 'y_label': '$y$', 'sgd_point': True, 'n_classes': 4}\n",
    "        error_opts = {'epoch': 5, 'x_label': '$t$', 'y_label': 'error'}\n",
    "\n",
    "        opts = {'contour_opts': contour_opts, 'error_opts': error_opts}\n",
    "        plot_helpers.classification_progression(X, Y, trajectory, indexes, classifier, \n",
    "                                                contour_plot=contour_plot, error_plot=error_plot, \n",
    "                                                options=opts)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        pass \n",
    "interact_manual(logistic_SGD, dataset=['linear', 'moons', 'circles', 'imbalanced'],\n",
    "                num_points=num_points_w, noise=noise_w, reg=reg_w, batch_size=batch_size_w, \n",
    "                lr=lr_w, num_iter=num_iter_w);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20, 15)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "def multi_class_lr(dataset):\n",
    "    # DATASET\n",
    "    X, y = get_classification_dataset(dataset, 200)\n",
    "    X = X[:, :2]\n",
    "    \n",
    "    # REGRESSION\n",
    "    model = LogisticRegression().fit(X, y)\n",
    "\n",
    "    # PREDICT\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    xy = np.c_[xx.ravel(), yy.ravel()]\n",
    "    C = model.predict(xy)\n",
    "    P = model.predict_proba(xy)\n",
    "    H = -(P * model.predict_log_proba(xy)).sum(axis=1)\n",
    "    \n",
    "    P = P.max(axis=1)\n",
    "    # Put the result into a color plot\n",
    "    C = C.reshape(xx.shape)\n",
    "    P = P.reshape(xx.shape)\n",
    "    H = H.reshape(xx.shape)\n",
    "    \n",
    "    # PLOTS\n",
    "    fig, axes = plt.subplots(2, 2)\n",
    "    axes[0, 0].set_title('Classification Boundary')\n",
    "    axes[0, 0].contourf(xx, yy, C, cmap=plt.cm.jet, alpha=0.5)\n",
    "    \n",
    "    axes[0, 1].set_title('Prediction Probabilities')\n",
    "    cf = axes[0, 1].contourf(xx, yy, P, cmap=plt.cm.cividis_r, alpha=0.5, vmin=1. / len(np.unique(y)), vmax=1)\n",
    "    m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "    m.set_array(P)\n",
    "    m.set_clim(1. / len(np.unique(y)), 1.)\n",
    "    cbar = plt.colorbar(m, ax=axes[0, 1])  \n",
    "    \n",
    "    axes[1, 0].set_title('Probabilistic Boundary')\n",
    "    axes[1, 0].contourf(xx, yy, P * C, cmap=plt.cm.jet, alpha=0.5)\n",
    "    \n",
    "    axes[1, 1].set_title('Entropy')\n",
    "    cf = axes[1, 1].contourf(xx, yy, H, cmap=plt.cm.cividis_r, alpha=0.5)\n",
    "    # Plot also the training points\n",
    "    \n",
    "    plt.colorbar(cf, ax=axes[1, 1])\n",
    "    for row in axes:\n",
    "        for ax in row:\n",
    "            ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.jet)\n",
    "\n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "    plt.show()\n",
    "    \n",
    "interact(multi_class_lr, dataset=['3-blobs', '4-blobs', 'iris', 'linear', 'imbalanced', '2-blobs',  'circles', 'moons']);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doubtful Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20, 6)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "def doubtful_logistic_regression(dataset, min_prob):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # DATASET\n",
    "    X, y = get_classification_dataset(dataset, 200)\n",
    "    X = X[:, :2]\n",
    "    \n",
    "    # REGRESSION\n",
    "    model = LogisticRegression().fit(X, y)\n",
    "    \n",
    "    # PREDICT\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    xy = np.c_[xx.ravel(), yy.ravel()]\n",
    "    P = model.predict_proba(xy)\n",
    "    C = 2 * model.predict(xy)\n",
    "    H = -(model.predict_log_proba(xy) * P).sum(axis=1)    \n",
    "    P = P.max(axis=1)\n",
    "\n",
    "    # Doubfult STEP\n",
    "    C[np.where(P < min_prob)[0]] = 1\n",
    "\n",
    "    C = C.reshape(xx.shape)\n",
    "    P = P.reshape(xx.shape)\n",
    "    H = H.reshape(xx.shape)\n",
    "    \n",
    "    # PLOTS\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].set_title('Classification Boundary')\n",
    "    axes[0].contourf(xx, yy, C, cmap=plt.cm.jet, alpha=0.5)\n",
    "    \n",
    "    axes[1].set_title('Probability')\n",
    "    cf = axes[1].contourf(xx, yy, P, cmap=plt.cm.cividis_r, alpha=0.5)\n",
    "    m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "    m.set_array(P)\n",
    "    m.set_clim(1. / len(np.unique(y)), 1.)\n",
    "    cbar = plt.colorbar(m, ax=axes[1])  \n",
    "    # Plot also the training points\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.jet)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "    plt.show()\n",
    "  \n",
    "    \n",
    "interact(doubtful_logistic_regression, dataset=['linear', 'imbalanced', '2-blobs', '3-blobs', '4-blobs', 'circles', 'moons', 'iris'],\n",
    "        min_prob=ipywidgets.FloatSlider(value=0.75, min=0.25, max=1, step=0.01, continuous_update=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Sensitive Classification (Logistic Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20,15)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "def cost_sensitive_logistic_regression(dataset, cost_ratio):\n",
    "    np.random.seed(0)\n",
    "    class_weight = {0: cost_ratio}\n",
    "\n",
    "    # DATASET\n",
    "    X, y = get_classification_dataset(dataset, 200)\n",
    "    X = X[:, :2]\n",
    "    \n",
    "    # REGRESSION\n",
    "    model = LogisticRegression(class_weight=class_weight).fit(X, y)\n",
    "\n",
    "    # PREDICT\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    xy = np.c_[xx.ravel(), yy.ravel()]\n",
    "    P = model.predict_proba(xy)\n",
    "    C = 2 * model.predict(xy)\n",
    "    H = -(model.predict_log_proba(xy) * P).sum(axis=1)    \n",
    "    P = P.max(axis=1)\n",
    "\n",
    "    \n",
    "    C = C.reshape(xx.shape)\n",
    "    P = P.reshape(xx.shape)\n",
    "    H = H.reshape(xx.shape)\n",
    "\n",
    "    # PLOTS\n",
    "    fig, axes = plt.subplots(2, 2)\n",
    "    axes[0, 0].set_title('Classification Boundary')\n",
    "    axes[0, 0].contourf(xx, yy, C, cmap=plt.cm.jet, alpha=0.5, vmin=0, vmax=1)\n",
    "\n",
    "    axes[0, 1].set_title('Prediction Probabilities')\n",
    "    cf = axes[0, 1].contourf(xx, yy, P, cmap=plt.cm.cividis_r, alpha=0.5, vmin=1. / len(np.unique(y)), vmax=1)\n",
    "    m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "    m.set_array(P)\n",
    "    m.set_clim(1. / len(np.unique(y)), 1.)\n",
    "    cbar = plt.colorbar(m, ax=axes[0, 1])  \n",
    "\n",
    "    axes[1, 0].set_title('Probabilistic Boundary')\n",
    "    axes[1, 0].contourf(xx, yy, P * C, cmap=plt.cm.jet, alpha=0.5, vmin=0, vmax=1)\n",
    "\n",
    "    axes[1, 1].set_title('Entropy')\n",
    "    cf = axes[1, 1].contourf(xx, yy, H, cmap=plt.cm.cividis_r, alpha=0.5)\n",
    "    # Plot also the training points\n",
    "\n",
    "    plt.colorbar(cf, ax=axes[1, 1])\n",
    "    for row in axes:\n",
    "        for ax in row:\n",
    "            ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.jet, vmin=0, vmax=1)\n",
    "\n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "    plt.show()\n",
    "\n",
    "interact(cost_sensitive_logistic_regression, \n",
    "         dataset=['linear', 'imbalanced', '2-blobs', 'circles', 'moons'],\n",
    "        cost_ratio=ipywidgets.FloatLogSlider(value=1, min=-3, max=4, step=0.1, continuous_update=False));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost-Sensitive Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "def cost_sensitive_linear_regression(dataset, over_estimation_cost_ratio, degree, alpha, n_samples, noise):\n",
    "    np.random.seed(42)\n",
    "    ratio = 1 / (1 + over_estimation_cost_ratio)\n",
    "    \n",
    "    # DATASET\n",
    "    w_star = np.array([1, 0.2, -0.3, 4])\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    _, f = get_regression_dataset(dataset, n_samples=200, X=X, noise=0, w=w_star)\n",
    "    _, y = get_regression_dataset(dataset, n_samples=200, X=X, noise=noise, w=w_star)\n",
    "\n",
    "    # REGRESSION\n",
    "    Phi = PolynomialFeatures(degree=degree, include_bias=True).fit_transform(np.atleast_2d(X).T)\n",
    "    w_hat = Ridge(alpha=alpha, fit_intercept=False).fit(Phi, y).coef_\n",
    "\n",
    "    # PREDICT\n",
    "    X_test = np.linspace(-1, 2, 100)\n",
    "    _, f_test = get_regression_dataset(dataset, n_samples=200, X=X_test, noise=0, w=w_star)\n",
    "    Phi_test = PolynomialFeatures(degree=degree, include_bias=True).fit_transform(np.atleast_2d(X_test).T)\n",
    "    y_equal = Phi_test @ w_hat\n",
    "    \n",
    "    # COST SENSITIVITY\n",
    "    y_sensitive = y_equal + noise * np.sqrt(2) * erfinv(2 * ratio - 1)\n",
    "    \n",
    "    # PLOT\n",
    "    plt.plot(X, y, '*')\n",
    "    plt.plot(X_test, y_sensitive, label='Cost Sensitive')\n",
    "    plt.plot(X_test, y_equal, label='Linear Regression')\n",
    "    plt.plot(X_test, f_test, label='True Function')\n",
    "    plt.legend(loc='upper left', ncol=4)\n",
    "\n",
    "    plt.ylim(-2, 2);\n",
    "    \n",
    "interact(cost_sensitive_linear_regression,  dataset=['cos', 'sinc', 'linear', 'linear-features'], \n",
    "         over_estimation_cost_ratio=ipywidgets.FloatLogSlider(value=0.1, min=-3, max=3, step=0.1, \n",
    "                                                              readout_format='.4f',\n",
    "                                      description='Ratio:', continuous_update=False),\n",
    "         n_samples=ipywidgets.IntSlider(value=30, min=30, max=1500, step=1, \n",
    "                                        description='N Samples:', continuous_update=False),\n",
    "         degree=ipywidgets.IntSlider(value=1, min=1, max=9, step=1, \n",
    "                                     description='Poly Degree:', continuous_update=False),\n",
    "         alpha=ipywidgets.BoundedFloatText(value=0, min=0, max=1000, step=0.0001, \n",
    "                                           description='Reg Coef.:', continuous_update=False),\n",
    "         noise=ipywidgets.FloatSlider(value=0.3, min=0, max=1, step=0.01, readout_format='.2f',\n",
    "                                      description='Noise level:', continuous_update=False)\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Sampling in Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (16, 5)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "queried_set = {}\n",
    "def uncertainty_sampling(dataset, criterion, noise):    \n",
    "    query_button = ipywidgets.Button(description=\"Query new point\")\n",
    "    update_button = ipywidgets.Button(description=\"Update Model\")\n",
    "    restart_button = ipywidgets.Button(description=\"Restart\")\n",
    "    X, Y = get_classification_dataset(dataset, 200, noise=noise)\n",
    "    num_classes = len(np.unique(Y)) - 1\n",
    "    X = X[:, :2]\n",
    "\n",
    "    indexes = np.arange(X.shape[0])\n",
    "    index_set = set([i for i in indexes])\n",
    "\n",
    "\n",
    "    def plot(model, X, Y, queried_set, next_idx=None, display_query=True):\n",
    "        neg_i = np.where(Y == 0)[0]\n",
    "        pos_i = np.where(Y == 1)[0]\n",
    "\n",
    "        queried_idx = [i for i in queried_set]\n",
    "        non_queried_idx = [i for i in index_set.difference(queried_set)]\n",
    "\n",
    "        qX, qY = X[queried_idx], Y[queried_idx]\n",
    "        nX, nY = X[non_queried_idx], Y[non_queried_idx]\n",
    "\n",
    "        # Model prediction contours.\n",
    "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "        h = .02 \n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        xy = np.c_[xx.ravel(), yy.ravel()]\n",
    "        P = model.predict_proba(xy).max(axis=1).reshape(xx.shape)\n",
    "        C = model.predict(xy).reshape(xx.shape)\n",
    "        H = -(model.predict_proba(xy) * model.predict_log_proba(xy)).sum(axis=1).reshape(xx.shape)\n",
    "        \n",
    "        # PLOTS\n",
    "        fig, axes = plt.subplots(1, 2)\n",
    "        axes[0].set_title('Classification Boundary')\n",
    "        axes[0].contourf(xx, yy, C, cmap=plt.cm.jet, alpha=0.5, vmin=0, vmax=num_classes)\n",
    "        \n",
    "        if criterion == 'max-entropy':\n",
    "            axes[1].set_title('Entropy')\n",
    "            cf = axes[1].contourf(xx, yy, H, cmap=plt.cm.cividis_r, alpha=0.5)\n",
    "            m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "            m.set_array(H)\n",
    "            cbar = plt.colorbar(m, ax=axes[1])  \n",
    "            cbar.set_label('Predicted Entropy', rotation=270, labelpad=20)\n",
    "            \n",
    "        elif criterion == 'min-probability':\n",
    "            axes[1].set_title('Probability')\n",
    "            cf = axes[1].contourf(xx, yy, P, cmap=plt.cm.cividis_r, alpha=0.5)\n",
    "            m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "            m.set_array(P)\n",
    "            cbar = plt.colorbar(m, ax=axes[1])  \n",
    "            cbar.set_label('Predicted Probability', rotation=270, labelpad=20)\n",
    "\n",
    "        # Plot also the training points\n",
    "        for ax in axes:\n",
    "            ax.scatter(qX[:, 0], qX[:, 1], c=qY, marker='o', s=200, cmap=plt.cm.jet, vmin=0, vmax=num_classes)\n",
    "            ax.scatter(nX[:, 0], nX[:, 1], c=nY, marker='o', alpha=0.3, s=20, cmap=plt.cm.jet, vmin=0, vmax=num_classes)\n",
    "            \n",
    "            if next_idx is not None:\n",
    "                ax.scatter(X[[next_idx], 0], X[[next_idx], 1], c=Y[[next_idx]], s=400, marker='*',\n",
    "                           cmap=plt.cm.jet, vmin=0, vmax=num_classes)\n",
    "            \n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "\n",
    "        \n",
    "        IPython.display.clear_output(wait=True)\n",
    "        IPython.display.display(plt.gcf())\n",
    "        plt.close()\n",
    "\n",
    "        if display_query:\n",
    "            display(query_button)\n",
    "        else:\n",
    "            display(update_button)\n",
    "        display(restart_button)\n",
    "\n",
    "    def update_model(b):\n",
    "        global queried_set, model\n",
    "\n",
    "        queried_idx = [i for i in queried_set]\n",
    "        model = LogisticRegression(C=10).fit(X[queried_idx], Y[queried_idx])\n",
    "\n",
    "        plot(model, X, Y, queried_set, next_idx=None, display_query=True)\n",
    "    \n",
    "    def restart(b):\n",
    "        global queried_set\n",
    "        queried_set = set()\n",
    "        classes = np.unique(Y)\n",
    "        for c in classes:\n",
    "            i = np.random.choice(np.where(Y == c)[0])\n",
    "            queried_set.add(i)\n",
    "        update_model(None)\n",
    "\n",
    "    def append_point(b):\n",
    "        global queried_set, model\n",
    "        \n",
    "        query_points = X\n",
    "        probs = model.predict_proba(X).max(axis=1)\n",
    "        H = model.predict_log_proba(X) * model.predict_proba(X)\n",
    "        H = H.sum(axis=1)\n",
    "\n",
    "        queried_idx = [i for i in queried_set]\n",
    "        probs[queried_idx] = float('Inf')\n",
    "        H[queried_idx] = float('Inf')\n",
    "\n",
    "        if criterion == 'max-entropy':\n",
    "            i = np.argmin(H)\n",
    "        elif criterion == 'min-probability':\n",
    "            i = np.argmin(probs)\n",
    "\n",
    "        plot(model, X, Y, queried_set,  i, display_query=False)\n",
    "        queried_set.add(i)\n",
    "\n",
    "    query_button.on_click(append_point)\n",
    "    update_button.on_click(update_model)\n",
    "    restart_button.on_click(restart)\n",
    "\n",
    "    restart(None);\n",
    "    \n",
    "interact(uncertainty_sampling, \n",
    "         dataset=['linear', 'imbalanced', '2-blobs', '3-blobs', '4-blobs', 'iris', 'circles', 'moons'], \n",
    "         criterion=['min-probability', 'max-entropy'],\n",
    "         noise=ipywidgets.FloatSlider(value=0.25, min=0, max=1, step=0.01, readout_format='.2f',\n",
    "                                      continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

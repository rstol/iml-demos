{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Code source: Sebastian Curi, Andreas Krause and Fanny Yang, based on Jaques Grobler (sklearn demos).\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# We start importing some modules and running some magic commands\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# General math and plotting modules.\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import erfinv\n",
    "\n",
    "# Project files.\n",
    "from utilities.util import gradient_descent\n",
    "from utilities.classifiers import Logistic\n",
    "from utilities.regressors import TStudent\n",
    "from utilities.regularizers import L2Regularizer\n",
    "from utilities.load_data import polynomial_data, linear_separable_data\n",
    "from utilities import plot_helpers\n",
    "\n",
    "# Widget and formatting modules\n",
    "import IPython\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual, fixed\n",
    "from matplotlib import rcParams\n",
    "# If in your browser the figures are not nicely vizualized, change the following line. \n",
    "rcParams['figure.figsize'] = (10, 5)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "# Machine Learning library. \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDRegressor, Ridge, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_dataset(dataset, X=None, n_samples=200, noise=0, w=None):\n",
    "    if X is None:\n",
    "        X = np.random.randn(n_samples)\n",
    "    \n",
    "    if dataset == 'cos':\n",
    "        Y = np.cos(1.5 * np.pi * X) + noise * np.random.randn(X.shape[0])\n",
    "        \n",
    "    elif dataset == 'sinc':\n",
    "        Y = X * np.sin(1.5 * np.pi * X) + noise * np.random.randn(X.shape[0])\n",
    "        \n",
    "    elif dataset == 'linear':\n",
    "        X = np.atleast_2d(X).T\n",
    "        Phi = PolynomialFeatures(degree=1, include_bias=True).fit_transform(X)\n",
    "        Y = Phi @ w[:2] + noise * np.random.randn(X.shape[0])\n",
    "    \n",
    "    elif dataset == 'linear-features':\n",
    "        X = np.atleast_2d(X).T\n",
    "        Phi = PolynomialFeatures(degree=len(w) - 1, include_bias=True).fit_transform(X)\n",
    "        Y = Phi @ w + noise * np.random.randn(X.shape[0])\n",
    "    \n",
    "    return X, Y\n",
    "    \n",
    "\n",
    "def get_classification_dataset(dataset, n_samples=200, noise=0.5):\n",
    "    if dataset == 'linear':\n",
    "        X, Y = linear_separable_data(n_samples, noise=noise, dim=2) \n",
    "        Y = (Y + 1) // 2\n",
    "    elif dataset == '2-blobs':\n",
    "        X, Y = datasets.make_classification(n_classes=2, n_features=2, n_informative=2, n_redundant=0,\n",
    "                                            n_clusters_per_class=1, n_samples=n_samples, random_state=8)\n",
    "    elif dataset == '3-blobs':\n",
    "        X, Y = datasets.make_classification(n_classes=3, n_features=2, n_informative=2, n_redundant=0,\n",
    "                                            n_clusters_per_class=1, n_samples=n_samples, random_state=8)\n",
    "    elif dataset == '4-blobs':\n",
    "        X, Y = datasets.make_classification(n_classes=4, n_features=2, n_informative=2, n_redundant=0,\n",
    "                                            n_clusters_per_class=1, n_samples=n_samples, random_state=8) \n",
    "    elif dataset == 'circles':\n",
    "        X, Y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "    elif dataset == 'moons':\n",
    "        X, Y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "    elif dataset == 'iris':\n",
    "        X, Y = datasets.load_iris(return_X_y=True)\n",
    "        X = X[:, :2]\n",
    "    elif dataset == 'imbalanced':\n",
    "        X, Y = linear_separable_data(n_samples, noise=noise, dim=2, num_negative=int(n_samples * 0.2))\n",
    "        Y = (Y + 1) // 2\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "# Probabilistic Classification (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7ca56039b94dfcae73e721a0596b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options=('linear', 'moons', 'circles', 'imbalanced'), vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = (20, 6)\n",
    "rcParams['font.size'] = 22\n",
    "\n",
    "num_points_w = ipywidgets.IntSlider(value=300, min=30, max=1500, step=1, description='Number of samples:',\n",
    "                                   style={'description_width': 'initial'}, continuous_update=False)\n",
    "noise_w = ipywidgets.FloatSlider(value=0.1, min=0, max=1, step=0.01, readout_format='.2f', description='Noise level:',\n",
    "                                 style={'description_width': 'initial'}, continuous_update=False)\n",
    "reg_w = ipywidgets.BoundedFloatText(value=0, min=0, max=1000, step=0.0001, description='Regularization:',\n",
    "                                    style={'description_width': 'initial'}, continuous_update=False)\n",
    "batch_size_w = ipywidgets.IntSlider(value=16, min=1, max=64, step=1, description='Batch Size:',\n",
    "                                   style={'description_width': 'initial'}, continuous_update=False)\n",
    "lr_w = ipywidgets.FloatLogSlider(value=0.3, min=-4, max=1, step=0.1, readout_format='.4f', description='Learning Rate:',\n",
    "                                 style={'description_width': 'initial'}, continuous_update=False)\n",
    "num_iter_w = ipywidgets.IntSlider(value=50, min=10, max=200, step=1, description='Num Iter:',\n",
    "                                   style={'description_width': 'initial'}, continuous_update=False)\n",
    "def logistic_SGD(dataset, num_points, noise, reg, batch_size, lr, num_iter):\n",
    "#     np.random.seed(42)\n",
    "    \n",
    "    # DATASET\n",
    "    X, Y = get_classification_dataset(dataset, num_points, noise)\n",
    "    Y = 2 * Y - 1 \n",
    "    if X.shape[1] == 2:\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X, ones), axis=-1)\n",
    "    \n",
    "    Xtest, Ytest = get_classification_dataset(dataset, int(0.1 * num_points), noise)\n",
    "    Ytest = 2 * Ytest - 1 \n",
    "    if Xtest.shape[1] == 2:\n",
    "        ones = np.ones((Xtest.shape[0], 1))\n",
    "        Xtest = np.concatenate((Xtest, ones), axis=-1)\n",
    "\n",
    "    indexes = np.arange(0, X.shape[0], 1)\n",
    "    np.random.shuffle(indexes)\n",
    "    X, Y = X[indexes], Y[indexes]\n",
    "\n",
    "    # REGRESSION\n",
    "    classifier = Logistic(X, Y)\n",
    "    classifier.load_test_data(Xtest, Ytest)\n",
    "    regularizer = L2Regularizer(reg)\n",
    "    np.random.seed(42)\n",
    "    w0 = np.random.randn(3, )\n",
    "    \n",
    "    opts = {'eta0': lr,\n",
    "            'n_iter': num_iter,\n",
    "            'batch_size': min(batch_size, X.shape[0]),\n",
    "            'n_samples': X.shape[0],\n",
    "            'algorithm': 'SGD',\n",
    "            }\n",
    "    \n",
    "    try:\n",
    "        trajectory, indexes = gradient_descent(w0, classifier, regularizer, opts)\n",
    "        \n",
    "        # PLOTS\n",
    "        contour_plot = plt.subplot(121)\n",
    "        error_plot = plt.subplot(122)\n",
    "\n",
    "        opt = {'marker': 'ro', 'fillstyle': 'full', 'label': '+ Train', 'size': 8}\n",
    "        plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=contour_plot, options=opt)\n",
    "        opt = {'marker': 'bs', 'fillstyle': 'full', 'label': '- Train', 'size': 8}\n",
    "        plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=contour_plot, options=opt)\n",
    "\n",
    "        opt = {'marker': 'ro', 'fillstyle': 'none', 'label': '+ Test', 'size': 8}\n",
    "        plot_helpers.plot_data(Xtest[np.where(Ytest == 1)[0], 0], Xtest[np.where(Ytest == 1)[0], 1], fig=contour_plot, options=opt)\n",
    "        opt = {'marker': 'bs', 'fillstyle': 'none', 'label': '- Test', 'size': 8}\n",
    "        plot_helpers.plot_data(Xtest[np.where(Ytest == -1)[0], 0], Xtest[np.where(Ytest == -1)[0], 1], fig=contour_plot, options=opt)\n",
    "\n",
    "        contour_opts = {'n_points': 100, 'x_label': '$x$', 'y_label': '$y$', 'sgd_point': True, 'n_classes': 4}\n",
    "        error_opts = {'epoch': 5, 'x_label': '$t$', 'y_label': 'error'}\n",
    "\n",
    "        opts = {'contour_opts': contour_opts, 'error_opts': error_opts}\n",
    "        plot_helpers.classification_progression(X, Y, trajectory, indexes, classifier, \n",
    "                                                contour_plot=contour_plot, error_plot=error_plot, \n",
    "                                                options=opts)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        pass \n",
    "interact_manual(logistic_SGD, dataset=['linear', 'moons', 'circles', 'imbalanced'],\n",
    "                num_points=num_points_w, noise=noise_w, reg=reg_w, batch_size=batch_size_w, \n",
    "                lr=lr_w, num_iter=num_iter_w);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost-Sensitive Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1aa7300512c434fa1d5dcef85a8046f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options=('cos', 'sinc', 'linear', 'linear-features'), vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.cost_sensitive_linear_regression(dataset, tau, degree, alpha, n_samples, noise)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "def cost_sensitive_linear_regression(dataset, tau, degree, alpha, n_samples, noise):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # DATASET\n",
    "    w_star = np.array([1, 0.2, -0.3, 4])\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    _, f = get_regression_dataset(dataset, n_samples=200, X=X, noise=0, w=w_star)\n",
    "    _, y = get_regression_dataset(dataset, n_samples=200, X=X, noise=noise, w=w_star)\n",
    "\n",
    "    # REGRESSION\n",
    "    Phi = PolynomialFeatures(degree=degree, include_bias=True).fit_transform(np.atleast_2d(X).T)\n",
    "    w_hat = Ridge(alpha=alpha, fit_intercept=False).fit(Phi, y).coef_\n",
    "\n",
    "    # PREDICT\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    _, f_test = get_regression_dataset(dataset, n_samples=200, X=X_test, noise=0, w=w_star)\n",
    "    Phi_test = PolynomialFeatures(degree=degree, include_bias=True).fit_transform(np.atleast_2d(X_test).T)\n",
    "    y_equal = Phi_test @ w_hat\n",
    "    \n",
    "    # COST SENSITIVITY\n",
    "    y_sensitive = y_equal + noise * np.sqrt(2) * erfinv(2*tau-1)\n",
    "    \n",
    "    # PLOT\n",
    "    p1=plt.plot(X, y, '*', label='Train samples')\n",
    "    p2=plt.plot(X_test, y_sensitive, label='Quantile Regression')\n",
    "    p3=plt.plot(X_test, y_equal, label='Linear Regression')\n",
    "    p4=plt.plot(X_test, f_test, label='True Function')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    #plt.legend(loc='upper left', ncol=1)\n",
    "\n",
    "    plt.ylim(-2, 2);\n",
    "    \n",
    "interact(cost_sensitive_linear_regression,  dataset=['cos', 'sinc', 'linear', 'linear-features'], \n",
    "         tau=ipywidgets.FloatSlider(value=0.5, min=0, max=1, step=0.001, \n",
    "                                                              readout_format='.4f',\n",
    "                                      description='Quantile:', continuous_update=False),\n",
    "         n_samples=ipywidgets.IntSlider(value=30, min=30, max=1500, step=1, \n",
    "                                        description='N Samples:', continuous_update=False),\n",
    "         degree=ipywidgets.IntSlider(value=1, min=1, max=9, step=1, \n",
    "                                     description='Poly Degree:', continuous_update=False),\n",
    "         alpha=ipywidgets.BoundedFloatText(value=0, min=0, max=1000, step=0.0001, \n",
    "                                           description='Reg Coef.:', continuous_update=False),\n",
    "         noise=ipywidgets.FloatSlider(value=0.3, min=0, max=1, step=0.01, readout_format='.2f',\n",
    "                                      description='Noise level:', continuous_update=False)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Sensitive Classification (Logistic Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a65724dcc243ac9e19a04e21a4173c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options=('linear', 'imbalanced', '2-blobs', 'moons'), vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = (20,8)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "def cost_sensitive_logistic_regression(dataset, cost_ratio):\n",
    "    # cost_ratio = cost_false_positive / cost_false_negative\n",
    "    np.random.seed(0)\n",
    "\n",
    "    min_positive_prob = 1 / (1 + cost_ratio)\n",
    "\n",
    "    # DATASET\n",
    "    X, y = get_classification_dataset(dataset, 200)\n",
    "    X = X[:, :2]\n",
    "    \n",
    "    # REGRESSION\n",
    "    model = LogisticRegression().fit(X, y)\n",
    "\n",
    "    # PREDICT\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    xy = np.c_[xx.ravel(), yy.ravel()]\n",
    "    P = model.predict_proba(xy)\n",
    "    C = 2 * model.predict(xy)\n",
    "    H = -(model.predict_log_proba(xy) * P).sum(axis=1)    \n",
    "    \n",
    "    # Cost Sensitive Step\n",
    "    C[np.where(P[:, 1] < min_positive_prob)[0]] = 0\n",
    "    C[np.where(P[:, 1] >= min_positive_prob)[0]] = 1\n",
    "    \n",
    "    P = P.max(axis=1)\n",
    "\n",
    "    C = C.reshape(xx.shape)\n",
    "    P = P.reshape(xx.shape)\n",
    "    H = H.reshape(xx.shape)\n",
    "    \n",
    "\n",
    "    # PLOTS\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].set_title('Classification Boundary')\n",
    "    axes[0].contourf(xx, yy, C, cmap=plt.cm.jet, alpha=0.5, vmin=0, vmax=1)\n",
    "\n",
    "    axes[1].set_title('Prediction Probabilities')\n",
    "    cf = axes[1].contourf(xx, yy, P, cmap=plt.cm.cividis_r, alpha=0.5, vmin=1. / len(np.unique(y)), vmax=1)\n",
    "    m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "    m.set_array(P)\n",
    "    m.set_clim(1. / len(np.unique(y)), 1.)\n",
    "    cbar = plt.colorbar(m, ax=axes[1])  \n",
    "\n",
    "    for ax in axes:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.jet, vmin=0, vmax=1)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "    plt.show()\n",
    "\n",
    "interact(cost_sensitive_logistic_regression, \n",
    "         dataset=['linear', 'imbalanced', '2-blobs', 'moons'],\n",
    "        cost_ratio=ipywidgets.FloatLogSlider(value=1, min=-3, max=4, step=0.1, continuous_update=False));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with abstention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2731bfe29ec5444a93c475f07ccd2fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options=('linear', 'imbalanced', '2-blobs', '3-blobs', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = (20, 6)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "def doubtful_logistic_regression(dataset, min_prob):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # DATASET\n",
    "    X, y = get_classification_dataset(dataset, 200)\n",
    "    X = X[:, :2]\n",
    "    \n",
    "    # REGRESSION\n",
    "    model = LogisticRegression().fit(X, y)\n",
    "    \n",
    "    # PREDICT\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    xy = np.c_[xx.ravel(), yy.ravel()]\n",
    "    P = model.predict_proba(xy)\n",
    "    C = 2 * model.predict(xy)\n",
    "    H = -(model.predict_log_proba(xy) * P).sum(axis=1)    \n",
    "    P = P.max(axis=1)\n",
    "\n",
    "    # Doubfult STEP\n",
    "    C[np.where(P < min_prob)[0]] = 1\n",
    "\n",
    "    C = C.reshape(xx.shape)\n",
    "    P = P.reshape(xx.shape)\n",
    "    H = H.reshape(xx.shape)\n",
    "    \n",
    "    # PLOTS\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].set_title('Classification Boundary')\n",
    "    axes[0].contourf(xx, yy, C, cmap=plt.cm.jet, alpha=0.5)\n",
    "    \n",
    "    axes[1].set_title('Probability')\n",
    "    cf = axes[1].contourf(xx, yy, P, cmap=plt.cm.cividis_r, alpha=0.5)\n",
    "    m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "    m.set_array(P)\n",
    "    m.set_clim(1. / len(np.unique(y)), 1.)\n",
    "    cbar = plt.colorbar(m, ax=axes[1])  \n",
    "    # Plot also the training points\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.jet)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "    plt.show()\n",
    "  \n",
    "    \n",
    "interact(doubtful_logistic_regression, dataset=['linear', 'imbalanced', '2-blobs', '3-blobs', '4-blobs', 'circles', 'moons', 'iris'],\n",
    "        min_prob=ipywidgets.FloatSlider(value=0.75, min=0.25, max=1, step=0.01, continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Source: Alexandru Tifrea and Fanny Yang, 2022.\n",
    "\n",
    "# Python Notebook Commands\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython import display\n",
    "\n",
    "display.display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# General math and plotting modules.\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.colors import DEFAULT_PLOTLY_COLORS\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Widget and formatting modules\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual, fixed, widgets\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "# Machine Learning library.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from utils import generate_data, generate_additional_data, repeat_experiment, compute_population_risk, get_error_vs_overparametrization\n",
    "from utils import compute_test_error as compute_empirical_risk\n",
    "\n",
    "import warnings\n",
    "\n",
    "rcParams['figure.figsize'] = (15, 6)\n",
    "rcParams['font.size'] = 20\n",
    "# Change these values if the images don't fit for your screen.\n",
    "figure_width = 1200\n",
    "figure_height = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Regularized Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with polynomial features\n",
    "\n",
    "In the following we show how the estimator depends on hyperparameters like the regularization coefficient (for LASSO and ridge penalties) or the the degree of the polynomial used for the features.\n",
    "\n",
    "\n",
    "Let's consider 1-dimensional data $\\{(x_i, y_i)\\}_{i=0}^n \\subset \\mathbb{R} \\times \\mathbb{R}$. To obtain a better feature representation for the data, we map the samples to the space of monomials of degree at most $d$, i.e. $\\varphi: \\mathbb{R} \\rightarrow span(\\{1, X, X^2, ..., X^d\\})$. The maximum degree controls the complexity of the regression model: the higher the degree, the more complex the features we obtain. As you will see later in the course, regression with polynomial features is equivalent to using a polynomial kernel function.\n",
    "\n",
    "We perform regularized regression and consider two different regularization penalties:\n",
    "\n",
    "- LASSO penalty, i.e. minimizing $L_{\\text{lasso}}(w; \\lambda) := \\sum_{i=0}^n (y_i - w^T\\varphi(x_i))^2 + \\lambda ||w||_1 $.\n",
    "\n",
    "- ridge penalty, i.e. minimizing $L_{\\text{ridge}}(w; \\lambda) := \\sum_{i=0}^n (y_i - w^T\\varphi(x_i))^2 + \\lambda ||w||_2^2 $.\n",
    "\n",
    "Below we show the mean squared error (MSE) computed on the training points, as well as on the test points. Moreover, we print the $\\ell_1$ and $\\ell_2$ norms of the trained estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955470c870f64b0fa7cda78ce32d44f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Ground truth function:', options=('sine', 'poly:0,-1,0,0,1'), styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def true_regression_fun(ground_truth):\n",
    "  if ground_truth == \"sine\":\n",
    "    return lambda X: np.cos(3 * np.pi * X)\n",
    "  elif \"poly\" in ground_truth:\n",
    "    coefficients = [float(coef) for coef in ground_truth.split(\":\")[1].split(\",\")]\n",
    "    return lambda X: np.poly1d(coefficients[::-1])(X)\n",
    "  else:\n",
    "    raise RuntimeError(f\"Unknown ground truth function {ground_truth}\")\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def poly_kernel_regression(ground_truth, n_samples, degree, reg_type, reg_coef, noise):\n",
    "    np.random.seed(101)\n",
    "\n",
    "    X = np.sort(np.random.uniform(0 - 1e-4, 1 + 1e-4, n_samples))\n",
    "    y = true_regression_fun(ground_truth)(X) + np.random.randn(n_samples) * noise\n",
    "\n",
    "    if reg_type == \"ridge\" and reg_coef > 0:\n",
    "      model = Ridge(alpha=reg_coef, fit_intercept=False, solver=\"svd\")\n",
    "      model_key = \"ridge\"\n",
    "    elif reg_type == \"lasso\" and reg_coef > 0:\n",
    "      model = Lasso(alpha=reg_coef, fit_intercept=False, tol=1e-2, max_iter=10000)\n",
    "      model_key = \"lasso\"\n",
    "    else:\n",
    "      model = LinearRegression(fit_intercept=False)\n",
    "      model_key = \"linearregression\"\n",
    "    \n",
    "    clf = make_pipeline(PolynomialFeatures(degree), model)\n",
    "    clf.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    X_test = np.random.uniform(0 - 1e-4, 1 + 1e-4, 100)\n",
    "    train_mse = mean_squared_error(\n",
    "      y_true=y,\n",
    "      y_pred=clf.predict(X[:, np.newaxis])\n",
    "    )\n",
    "    test_mse = mean_squared_error(\n",
    "      y_true=true_regression_fun(ground_truth)(X_test) + np.random.randn(100) * noise,\n",
    "      y_pred=clf.predict(X_test[:, np.newaxis])\n",
    "    )\n",
    "    \n",
    "    all_X = np.sort(np.concatenate((X_test, X)))\n",
    "    fig = make_subplots(rows=1, cols=2) #, row_width=[0.15, 0.35])\n",
    "    fig.add_trace(go.Scatter(x=all_X,\n",
    "                             y=clf.predict(all_X[:, np.newaxis]),\n",
    "                             line_width=3,\n",
    "                             name=\"Model\"),\n",
    "                  row=1,\n",
    "                  col=1)\n",
    "    fig.add_trace(go.Scatter(x=all_X,\n",
    "                             y=true_regression_fun(ground_truth)(all_X),\n",
    "                             line_dash=\"dash\",\n",
    "                             line_width=3,\n",
    "                             name=\"True function\"),\n",
    "                  row=1,\n",
    "                  col=1)\n",
    "    fig.add_trace(go.Scatter(x=X,\n",
    "                             y=y,\n",
    "                             mode=\"markers\",\n",
    "                             marker_size=7,\n",
    "                             marker_symbol=\"x\",\n",
    "                             marker_color=\"black\",\n",
    "                             name=\"Samples\"),\n",
    "                  row=1,\n",
    "                  col=1)\n",
    "    fig.add_trace(go.Scatter(x=np.arange(clf[model_key].coef_.shape[0]),\n",
    "                             y=np.fabs(clf[model_key].coef_),\n",
    "                             line_width=3,\n",
    "                             showlegend=False),\n",
    "                  row=1,\n",
    "                  col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Training MSE = {train_mse:.6}\" + \"<br>Test MSE\" + f\" = {test_mse:.6}\" + \"<br>\" + f\"l2 norm = {np.linalg.norm(clf[model_key].coef_):.2}; l1 norm = {np.linalg.norm(clf[model_key].coef_, ord=1):.2}\",\n",
    "        height=figure_height,\n",
    "        width=figure_width,\n",
    "        margin=go.layout.Margin(\n",
    "            l=0,  #left margin\n",
    "            r=0,  #right margin\n",
    "            b=0,  #bottom margin\n",
    "            t=100,  #top margin\n",
    "        ),\n",
    "        xaxis1_range=[0, 1],\n",
    "        xaxis1_title=\"x\",\n",
    "        yaxis1_range=[-2, 2],\n",
    "        yaxis1_title=\"y\",\n",
    "        xaxis2_title=\"Degree\",\n",
    "        yaxis2_title=\"Abs. value of coefficient\",\n",
    "    )\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.97,\n",
    "        xanchor=\"left\",\n",
    "        x=0.37\n",
    "    ))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "_ = interact(\n",
    "    poly_kernel_regression,\n",
    "    ground_truth=ipywidgets.Dropdown(options=[\"sine\", \"poly:0,-1,0,0,1\"],\n",
    "                                     value=\"sine\",\n",
    "                                     description='Ground truth function:',\n",
    "                                     disabled=False,\n",
    "                                     style={'description_width': 'initial'},\n",
    "                                     continuous_update=False),\n",
    "    n_samples=ipywidgets.IntSlider(value=20,\n",
    "                                   min=5,\n",
    "                                   max=100,\n",
    "                                   step=5,\n",
    "                                   description='Number of samples:',\n",
    "                                   style={'description_width': 'initial'},\n",
    "                                   continuous_update=False),\n",
    "    degree=ipywidgets.IntSlider(value=5,\n",
    "                                min=1,\n",
    "                                max=15,\n",
    "                                step=1,\n",
    "                                description='Polynomial Degree:',\n",
    "                                style={'description_width': 'initial'},\n",
    "                                continuous_update=False),\n",
    "    reg_type=ipywidgets.Dropdown(options=[\"lasso\", \"ridge\"],\n",
    "                                 value=\"ridge\",\n",
    "                                 description='Regularization type:',\n",
    "                                 disabled=False,\n",
    "                                 style={'description_width': 'initial'},\n",
    "                                 continuous_update=False),\n",
    "    reg_coef=ipywidgets.FloatSlider(value=0.,\n",
    "                                    min=0,\n",
    "                                    max=0.001,\n",
    "                                    step=0.0001,\n",
    "                                    readout_format='.4f',\n",
    "                                    description='Regularization coefficient:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),\n",
    "    noise=ipywidgets.FloatSlider(value=0.5,\n",
    "                                 min=0,\n",
    "                                 max=1,\n",
    "                                 step=0.1,\n",
    "                                 readout_format='.2f',\n",
    "                                 description='Noise level:',\n",
    "                                 style={'description_width': 'initial'},\n",
    "                                 continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of noise on the estimator norm\n",
    "\n",
    "In these figures we see how increasing the noise level leads to linear estimators whose weights have higher norm. This phenomenon holds true for both the $\\ell_1$ and the $\\ell_2$ norm. This observation motivates regularization with a norm-based penalty (like LASSO or ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa56c34dfdd41a8b625e9703e02db07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, continuous_update=False, description='Number of samples:', min=10, s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def norm_increase_with_noise(n_samples, degree, reg_type):\n",
    "    np.random.seed(101)\n",
    "    \n",
    "    ground_truth = \"poly:0,-1,0,0,1\"\n",
    "    noise_values = np.arange(0, 0.1, 0.01)\n",
    "    reg_coef_values = [0., 1e-2, 1e-1, 1]\n",
    "    \n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    gauss_noise = np.random.randn(n_samples)\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    for i, reg_coef in enumerate(reg_coef_values):\n",
    "      l1_norms, l2_norms = [], []\n",
    "      if reg_type == \"ridge\" and reg_coef > 0:\n",
    "        model = Ridge(alpha=reg_coef, fit_intercept=False, solver=\"svd\")\n",
    "        model_key = \"ridge\"\n",
    "      elif reg_type == \"lasso\" and reg_coef > 0:\n",
    "        model = Lasso(alpha=reg_coef, fit_intercept=False, tol=1e-2, max_iter=10000)\n",
    "        model_key = \"lasso\"\n",
    "      else:\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        model_key = \"linearregression\"\n",
    "      \n",
    "      for noise in noise_values:\n",
    "        y = true_regression_fun(ground_truth)(X) + gauss_noise * noise\n",
    "        clf = make_pipeline(PolynomialFeatures(degree), model)\n",
    "        clf.fit(X[:, np.newaxis], y)\n",
    "        l1_norms.append(np.linalg.norm(clf[model_key].coef_, ord=1))\n",
    "        l2_norms.append(np.linalg.norm(clf[model_key].coef_, ord=2))\n",
    "    \n",
    "      fig.add_trace(go.Scatter(x=noise_values,\n",
    "                               y=l1_norms,\n",
    "                               line_width=3,\n",
    "                               marker_color=DEFAULT_PLOTLY_COLORS[i],\n",
    "                               name=f\"${reg_type.title()}\\ \\lambda={reg_coef}$\",\n",
    "                               legendgroup=f\"${reg_type.title()}\\ \\lambda={reg_coef}$\"),\n",
    "                    row=1,\n",
    "                    col=1)\n",
    "      fig.add_trace(go.Scatter(x=noise_values,\n",
    "                               y=l2_norms,\n",
    "                               line_width=3,\n",
    "                               marker_color=DEFAULT_PLOTLY_COLORS[i],\n",
    "                               name=f\"${reg_type.title()}\\ \\lambda={reg_coef}$\",\n",
    "                               legendgroup=f\"${reg_type.title()}\\ \\lambda={reg_coef}$\",\n",
    "                               showlegend=False),\n",
    "                    row=1,\n",
    "                    col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=figure_height,\n",
    "        width=figure_width,\n",
    "        margin=go.layout.Margin(\n",
    "            l=0,  #left margin\n",
    "            r=0,  #right margin\n",
    "            b=0,  #bottom margin\n",
    "            t=10,  #top margin\n",
    "        ),\n",
    "        xaxis1_title=\"Noise level\",\n",
    "        yaxis1_title=\"$\\ell_1\\ norm$\",\n",
    "        xaxis2_title=\"Noise level\",\n",
    "        yaxis2_title=\"$\\ell_2\\ norm$\",\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "_ = interact(\n",
    "    norm_increase_with_noise,\n",
    "    n_samples=ipywidgets.IntSlider(value=20,\n",
    "                                   min=10,\n",
    "                                   max=100,\n",
    "                                   step=10,\n",
    "                                   description='Number of samples:',\n",
    "                                   style={'description_width': 'initial'},\n",
    "                                   continuous_update=False),\n",
    "    degree=ipywidgets.IntSlider(value=5,\n",
    "                                min=1,\n",
    "                                max=10,\n",
    "                                step=1,\n",
    "                                description='Polynomial Degree:',\n",
    "                                style={'description_width': 'initial'},\n",
    "                                continuous_update=False),\n",
    "    reg_type=ipywidgets.Dropdown(options=[\"lasso\", \"ridge\"],\n",
    "                                 value=\"ridge\",\n",
    "                                 description='Regularization type:',\n",
    "                                 disabled=False,\n",
    "                                 style={'description_width': 'initial'},\n",
    "                                 continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Bias-variance trade-off for ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the bias and the variance of an estimator by sampling different training sets, and using a hold-out validation set to compute its empirical error.\n",
    "\n",
    "The stacked area plot below illustrates the decomposition of the risk into three terms: the squared bias, the variance, and irreducible noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1726233adfe840818f5a715d371a2452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=200.0, continuous_update=False, description='Number of samples:', max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_size = 1000\n",
    "num_trials = 5\n",
    "snr = 1\n",
    "\n",
    "all_noise_sigmas = [0, 0.1, 0.5, 1]\n",
    "\n",
    "\n",
    "def plot_bias_variance_for_ridge(n, d, noise_sigma):\n",
    "    ridge_coefficients = np.arange(0, 20, 1)\n",
    "    risks, squared_biases, variances = [], [], []\n",
    "    n, d = int(n), int(d)\n",
    "\n",
    "    # Sample the validation set and one traing set for each of the trials.\n",
    "    X_validation, y_validation, beta_star, Sigma = generate_data(\n",
    "        n=validation_size, d=d, snr=snr, noise_sigma=noise_sigma)\n",
    "    all_X, all_y = generate_additional_data(num_samples=n * num_trials,\n",
    "                                            d=d,\n",
    "                                            Sigma=Sigma,\n",
    "                                            beta_star=beta_star,\n",
    "                                            noise_sigma=noise_sigma)\n",
    "    for ridge_coef in ridge_coefficients:\n",
    "        validation_predictions, validation_bayes_predictions = [], []\n",
    "\n",
    "        # Train num_trials estimators and use the validation set to estimate the bias and variance.\n",
    "        for i in range(num_trials):\n",
    "            start, end = i * n, (i + 1) * n\n",
    "            # Compute closed form solution of the ridge regression optimization problem.\n",
    "            beta_hat = 1 / (1 + ridge_coef) * np.linalg.inv(\n",
    "                all_X[start:end].T\n",
    "                @ all_X[start:end]) @ all_X[start:end].T @ all_y[start:end]\n",
    "            validation_predictions.append(\n",
    "                (X_validation @ beta_hat).reshape(-1, 1))\n",
    "            validation_bayes_predictions.append(\n",
    "                (X_validation @ beta_star).reshape(-1, 1))\n",
    "\n",
    "        validation_predictions, validation_bayes_predictions = np.array(\n",
    "            validation_predictions), np.array(validation_bayes_predictions)\n",
    "        risks.append(np.power(validation_predictions - y_validation, 2).mean())\n",
    "        variances.append(\n",
    "            np.power(\n",
    "                validation_predictions - validation_predictions.mean(axis=0),\n",
    "                2).mean())\n",
    "        squared_biases.append(\n",
    "            max(0, risks[-1] - variances[-1] - noise_sigma**2))\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=ridge_coefficients,\n",
    "                   y=np.ones_like(ridge_coefficients) * noise_sigma**2,\n",
    "                   name=\"Irreducible noise\",\n",
    "                   marker_color=\"gray\"))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=ridge_coefficients, y=variances, name=\"Variance\"))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=ridge_coefficients,\n",
    "                   y=squared_biases,\n",
    "                   name=\"Bias<sup>2</sup>\"))\n",
    "    fig.add_trace(go.Scatter(x=ridge_coefficients, y=risks, name=\"Risk\"))\n",
    "\n",
    "    if noise_sigma == 0.5:\n",
    "        yaxis_range = [0, 1.5]\n",
    "    elif noise_sigma == 1:\n",
    "        yaxis_range = [0, 2.2]\n",
    "    else:\n",
    "        yaxis_range = [0, 1.05]\n",
    "\n",
    "    fig.update_layout(height=figure_height,\n",
    "                      width=figure_width,\n",
    "                      yaxis_range=yaxis_range,\n",
    "                      yaxis_title=\"Risk / Bias / Variance\",\n",
    "                      xaxis_title=\"Ridge coefficient\",\n",
    "                      hovermode='x')\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "_ = interact(\n",
    "    plot_bias_variance_for_ridge,\n",
    "    n=ipywidgets.FloatSlider(value=200,\n",
    "                             min=100,\n",
    "                             max=500,\n",
    "                             step=10,\n",
    "                             readout_format='d',\n",
    "                             description='Number of samples:',\n",
    "                             style={'description_width': 'initial'},\n",
    "                             continuous_update=False),\n",
    "    d=ipywidgets.FloatSlider(value=100,\n",
    "                             min=10,\n",
    "                             max=100,\n",
    "                             step=10,\n",
    "                             readout_format='d',\n",
    "                             description='Data dimension:',\n",
    "                             style={'description_width': 'initial'},\n",
    "                             continuous_update=False),\n",
    "    noise_sigma=ipywidgets.Dropdown(options=all_noise_sigmas,\n",
    "                                    value=0.5,\n",
    "                                    description='Noise level:',\n",
    "                                    disabled=False,\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of overparametrization for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Dg_cFNDx4_Ul"
   },
   "outputs": [],
   "source": [
    "def plot_risk_vs_overparametrization(n=None,\n",
    "                                     d=None,\n",
    "                                     cov_type=\"isotropic\",\n",
    "                                     num_runs=1):\n",
    "    assert (n is not None) or (d is not None)\n",
    "\n",
    "    all_snr = [1]\n",
    "    all_gammas = np.concatenate((\n",
    "        np.arange(0.1, 2, 0.1),\n",
    "        # Uncomment this line for higher overparameterization, but at the cost of\n",
    "        # longer computation time.\n",
    "        # np.arange(3, 10)\n",
    "    ))\n",
    "    params = {\n",
    "        \"all_gammas\": all_gammas,\n",
    "        \"all_snr\": all_snr,\n",
    "        \"fix_n_vary_d\": (n is not None),\n",
    "        \"cov_type\": cov_type,\n",
    "        \"use_ridge\": True,\n",
    "    }\n",
    "    if n is not None:\n",
    "        params[\"n\"] = n\n",
    "    else:\n",
    "        params[\"d\"] = d\n",
    "    ridge_aggregated_risks = repeat_experiment(\n",
    "        num_runs, get_error_vs_overparametrization, params)\n",
    "    params[\"use_ridge\"] = False\n",
    "    ridgeless_aggregated_risks = repeat_experiment(\n",
    "        num_runs, get_error_vs_overparametrization, params)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for snr in all_snr:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=all_gammas,\n",
    "                       y=ridge_aggregated_risks[snr],\n",
    "                       name=\"Ridge regression\"))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=all_gammas,\n",
    "                       y=ridgeless_aggregated_risks[snr],\n",
    "                       name=\"Ridgeless regression\"))\n",
    "    fig.update_layout(\n",
    "        height=figure_height,\n",
    "        width=figure_width,\n",
    "        yaxis_type=\"log\",\n",
    "        yaxis_range=[-1, 2.5],\n",
    "        yaxis_title=\"Population risk\",\n",
    "        xaxis_type=\"log\",\n",
    "        xaxis_title=\"$\\Large\\gamma=d/n$\",\n",
    "        title=f\"Fixed n={int(n)}\"\n",
    "        if params[\"fix_n_vary_d\"] else f\"Fixed d={int(d)}\",\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtey1_FGA4ww"
   },
   "source": [
    "### Select the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613,
     "referenced_widgets": [
      "288c9d0e04b346bd997ed5fa8e57c72f",
      "a92ae6827ceb4d7bbfd54131326f8fc4",
      "a58251239bb444ecb1057b30905d3751",
      "04e588bd63d64af89d8bcd186047ca48",
      "d2d38b322c894715842884e2baa91c44",
      "7d6cb85d737847588f241770507d9c97",
      "5f0c64edb0a1448580a3827387c2d283",
      "e2740951b7254f66815bef6a46e850df",
      "ab4c6402ecc04adebb4632867404117b",
      "808ed231a0854339bc4f1ea1a8ec84a0",
      "8f83a7ba43f049359fc91236d627df36",
      "401367351a0443a59f25a5eed6ff3ed1",
      "5eee206569dc42c0b9546ce108a8235a"
     ]
    },
    "id": "H0BM-sHJA4A7",
    "outputId": "89b5c5d6-249f-41ea-d8e8-546146917aff",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5c667e44d640dd8892f27fcb5de115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=100.0, continuous_update=False, description='Number of samples:', max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = interact(\n",
    "    lambda n, cov_type, num_runs: plot_risk_vs_overparametrization(\n",
    "        n=n, cov_type=cov_type, num_runs=num_runs),\n",
    "    n=ipywidgets.FloatSlider(value=100,\n",
    "                             min=100,\n",
    "                             max=200,\n",
    "                             step=10,\n",
    "                             readout_format='d',\n",
    "                             description='Number of samples:',\n",
    "                             style={'description_width': 'initial'},\n",
    "                             continuous_update=False),\n",
    "    cov_type=ipywidgets.Dropdown(options=[\"isotropic\", \"misspecified\"],\n",
    "                                 value=\"isotropic\",\n",
    "                                 description='Covariance model:',\n",
    "                                 disabled=False,\n",
    "                                 style={'description_width': 'initial'},\n",
    "                                 continuous_update=False),\n",
    "    num_runs=ipywidgets.Dropdown(options=[1, 3, 5, 10],\n",
    "                                 value=1,\n",
    "                                 description='Number of experiments:',\n",
    "                                 disabled=False,\n",
    "                                 style={'description_width': 'initial'},\n",
    "                                 continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6eqYP8YBBDa"
   },
   "source": [
    "### Select the dimension of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613,
     "referenced_widgets": [
      "faecb0a4bf5b4241b285b079e214ca5d",
      "e2405df6912844108ae6b7dd0cca26bf",
      "4a1aff544d1b4789936909fa9d1c00e5",
      "5b6b4773958845b0a9c5264e65fcda85",
      "b999849e41984e7bab5ba29944e5e885",
      "1665c5bb69364a0b86a0f02cbb6d3bdb",
      "a1560d8790ec458fa4b2ca7233c4db70",
      "b7892ff133b24d44b9069fb3d75ca2a6",
      "fadc638e6a1b4072ad479c1420e52d7c",
      "577a32cc2e024989a55279c4ce8e7265",
      "1ce75972e4a545bc8720b11d36f3bd7b",
      "93c8be6f9ca54a30a843017147e5f404",
      "481a0a80f1db44d9a1b7d97cd95260d3"
     ]
    },
    "id": "6uIB3gCLA34A",
    "outputId": "c11cce76-a710-4d9b-d1df-e45a9c266e70"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc18a3a72c2b43aab0d6935e43ba0469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=20.0, continuous_update=False, description='Data dimension:', max=1000…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = interact(\n",
    "    lambda d, cov_type, num_runs: plot_risk_vs_overparametrization(\n",
    "        d=d, cov_type=cov_type, num_runs=num_runs),\n",
    "    d=ipywidgets.FloatSlider(value=20,\n",
    "                             min=20,\n",
    "                             max=1000,\n",
    "                             step=10,\n",
    "                             readout_format='d',\n",
    "                             description='Data dimension:',\n",
    "                             style={'description_width': 'initial'},\n",
    "                             continuous_update=False),\n",
    "    cov_type=ipywidgets.Dropdown(options=[\"isotropic\", \"misspecified\"],\n",
    "                                 value=\"isotropic\",\n",
    "                                 description='Covariance model:',\n",
    "                                 disabled=False,\n",
    "                                 style={'description_width': 'initial'},\n",
    "                                 continuous_update=False),\n",
    "    num_runs=ipywidgets.Dropdown(options=[1, 3, 5, 10],\n",
    "                                 value=1,\n",
    "                                 description='Number of experiments:',\n",
    "                                 disabled=False,\n",
    "                                 style={'description_width': 'initial'},\n",
    "                                 continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

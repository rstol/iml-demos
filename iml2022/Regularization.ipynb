{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Source: Alexandru Tifrea and Fanny Yang, 2021.\n",
    "# Based on an earlier version by Sebastian Curi and Andreas Krause.\n",
    "\n",
    "# Python Notebook Commands\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from IPython import display\n",
    "\n",
    "display.display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# General math and plotting modules.\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.colors import DEFAULT_PLOTLY_COLORS\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Widget and formatting modules\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual, fixed, widgets\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "# Machine Learning library.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "\n",
    "rcParams['figure.figsize'] = (15, 6)\n",
    "rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with polynomial features\n",
    "\n",
    "In the following we show how the estimator depends on hyperparameters like the regularization coefficient (for LASSO and ridge penalties) or the the degree of the polynomial used for the features.\n",
    "\n",
    "\n",
    "Let's consider 1-dimensional data $\\{(x_i, y_i)\\}_{i=0}^n \\subset \\mathbb{R} \\times \\mathbb{R}$. To obtain a better feature representation for the data, we map the samples to the space of monomials of degree at most $d$, i.e. $\\varphi: \\mathbb{R} \\rightarrow span(\\{1, X, X^2, ..., X^d\\})$. The maximum degree controls the complexity of the regression model: the higher the degree, the more complex the features we obtain. As you will see later in the course, regression with polynomial features is equivalent to using a polynomial kernel function.\n",
    "\n",
    "We perform regularized regression and consider two different regularization penalties:\n",
    "\n",
    "- LASSO penalty, i.e. minimizing $L_{\\text{lasso}}(w; \\lambda) := \\sum_{i=0}^n (y_i - w^T\\varphi(x_i))^2 + \\lambda ||w||_1 $.\n",
    "\n",
    "- ridge penalty, i.e. minimizing $L_{\\text{ridge}}(w; \\lambda) := \\sum_{i=0}^n (y_i - w^T\\varphi(x_i))^2 + \\lambda ||w||_2^2 $.\n",
    "\n",
    "Below we show the mean squared error (MSE) computed on the training points, as well as the L2 error of the estimator compared to the ground truth function $f^*$, i.e. $||\\hat{f}-f^*||_{L_2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586af148dac4460ca5d9cc66e2991ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Ground truth function:', options=('sine', 'poly:0,-1,0,0,1'), stylâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def true_regression_fun(ground_truth):\n",
    "  if ground_truth == \"sine\":\n",
    "    return lambda X: np.cos(3 * np.pi * X)\n",
    "  elif \"poly\" in ground_truth:\n",
    "    coefficients = [float(coef) for coef in ground_truth.split(\":\")[1].split(\",\")]\n",
    "    return lambda X: np.poly1d(coefficients[::-1])(X)\n",
    "  else:\n",
    "    raise RuntimeError(f\"Unknown ground truth function {ground_truth}\")\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def poly_kernel_regression(ground_truth, n_samples, degree, reg_type, reg_coef, noise):\n",
    "    np.random.seed(101)\n",
    "\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = true_regression_fun(ground_truth)(X) + np.random.randn(n_samples) * noise\n",
    "\n",
    "    if reg_type == \"ridge\" and reg_coef > 0:\n",
    "      model = Ridge(alpha=reg_coef, fit_intercept=False, solver=\"svd\")\n",
    "      model_key = \"ridge\"\n",
    "    elif reg_type == \"lasso\" and reg_coef > 0:\n",
    "      model = Lasso(alpha=reg_coef, fit_intercept=False, tol=1e-2, max_iter=10000)\n",
    "      model_key = \"lasso\"\n",
    "    else:\n",
    "      model = LinearRegression(fit_intercept=False)\n",
    "      model_key = \"linearregression\"\n",
    "    \n",
    "    clf = make_pipeline(PolynomialFeatures(degree), model)\n",
    "    clf.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    X_test = np.sort(np.concatenate((np.linspace(0 - 1e-4, 1 + 1e-4, 100), X)))\n",
    "    train_mse = mean_squared_error(\n",
    "      y_true=y,\n",
    "      y_pred=clf.predict(X[:, np.newaxis])\n",
    "    )\n",
    "    test_mse = mean_squared_error(\n",
    "      y_true=true_regression_fun(ground_truth)(X_test),\n",
    "      y_pred=clf.predict(X_test[:, np.newaxis])\n",
    "    )\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2) #, row_width=[0.15, 0.35])\n",
    "    fig.add_trace(go.Scatter(x=X_test,\n",
    "                             y=clf.predict(X_test[:, np.newaxis]),\n",
    "                             line_width=3,\n",
    "                             name=\"Model\"),\n",
    "                  row=1,\n",
    "                  col=1)\n",
    "    fig.add_trace(go.Scatter(x=X_test,\n",
    "                             y=true_regression_fun(ground_truth)(X_test),\n",
    "                             line_dash=\"dash\",\n",
    "                             line_width=3,\n",
    "                             name=\"True function\"),\n",
    "                  row=1,\n",
    "                  col=1)\n",
    "    fig.add_trace(go.Scatter(x=X,\n",
    "                             y=y,\n",
    "                             mode=\"markers\",\n",
    "                             marker_size=7,\n",
    "                             marker_symbol=\"x\",\n",
    "                             marker_color=\"black\",\n",
    "                             name=\"Samples\"),\n",
    "                  row=1,\n",
    "                  col=1)\n",
    "    fig.add_trace(go.Scatter(x=np.arange(clf[model_key].coef_.shape[0]),\n",
    "                             y=np.fabs(clf[model_key].coef_),\n",
    "                             line_width=3,\n",
    "                             showlegend=False),\n",
    "                  row=1,\n",
    "                  col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Training MSE = {train_mse:.6}\" + \"<br>L2 error\" + f\" = {test_mse:.6}\" + \"<br>\" + f\"l2 norm = {np.linalg.norm(clf[model_key].coef_):.2}; l1 norm = {np.linalg.norm(clf[model_key].coef_, ord=1):.2}\",\n",
    "        margin=go.layout.Margin(\n",
    "            l=0,  #left margin\n",
    "            r=0,  #right margin\n",
    "            b=0,  #bottom margin\n",
    "            t=100,  #top margin\n",
    "        ),\n",
    "        xaxis1_range=[0, 1],\n",
    "        xaxis1_title=\"x\",\n",
    "        yaxis1_range=[-2, 2],\n",
    "        yaxis1_title=\"y\",\n",
    "        xaxis2_title=\"Degree\",\n",
    "        yaxis2_title=\"Abs. value of coefficient\",\n",
    "    )\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.97,\n",
    "        xanchor=\"left\",\n",
    "        x=0.37\n",
    "    ))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "_ = interact(\n",
    "    poly_kernel_regression,\n",
    "    ground_truth=ipywidgets.Dropdown(options=[\"sine\", \"poly:0,-1,0,0,1\"],\n",
    "                                     value=\"sine\",\n",
    "                                     description='Ground truth function:',\n",
    "                                     disabled=False,\n",
    "                                     style={'description_width': 'initial'},\n",
    "                                     continuous_update=False),\n",
    "    n_samples=ipywidgets.IntSlider(value=20,\n",
    "                                   min=5,\n",
    "                                   max=100,\n",
    "                                   step=5,\n",
    "                                   description='Number of samples:',\n",
    "                                   style={'description_width': 'initial'},\n",
    "                                   continuous_update=False),\n",
    "    degree=ipywidgets.IntSlider(value=5,\n",
    "                                min=1,\n",
    "                                max=15,\n",
    "                                step=1,\n",
    "                                description='Polynomial Degree:',\n",
    "                                style={'description_width': 'initial'},\n",
    "                                continuous_update=False),\n",
    "    reg_type=ipywidgets.Dropdown(options=[\"lasso\", \"ridge\"],\n",
    "                                 value=\"ridge\",\n",
    "                                 description='Regularization type:',\n",
    "                                 disabled=False,\n",
    "                                 style={'description_width': 'initial'},\n",
    "                                 continuous_update=False),\n",
    "    reg_coef=ipywidgets.FloatSlider(value=0.,\n",
    "                                    min=0,\n",
    "                                    max=0.001,\n",
    "                                    step=0.0001,\n",
    "                                    readout_format='.4f',\n",
    "                                    description='Regularization coefficient:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),\n",
    "    noise=ipywidgets.FloatSlider(value=0.5,\n",
    "                                 min=0,\n",
    "                                 max=1,\n",
    "                                 step=0.1,\n",
    "                                 readout_format='.2f',\n",
    "                                 description='Noise level:',\n",
    "                                 style={'description_width': 'initial'},\n",
    "                                 continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of noise on the estimator norm\n",
    "\n",
    "In these figures we see how increasing the noise level leads to linear estimators whose weights have higher norm. This phenomenon holds true for both the $\\ell_1$ and the $\\ell_2$ norm. This observation motivates regularization with a norm-based penalty (like LASSO or ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f4a7ae9800491ea802c633fbc3256b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, continuous_update=False, description='Number of samples:', min=10, sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def norm_increase_with_noise(n_samples, degree, reg_type):\n",
    "    np.random.seed(101)\n",
    "    \n",
    "    ground_truth = \"poly:0,-1,0,0,1\"\n",
    "    noise_values = np.arange(0, 0.1, 0.01)\n",
    "    reg_coef_values = [0., 1e-2, 1e-1, 1]\n",
    "    \n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    gauss_noise = np.random.randn(n_samples)\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    for i, reg_coef in enumerate(reg_coef_values):\n",
    "      l1_norms, l2_norms = [], []\n",
    "      if reg_type == \"ridge\" and reg_coef > 0:\n",
    "        model = Ridge(alpha=reg_coef, fit_intercept=False, solver=\"svd\")\n",
    "        model_key = \"ridge\"\n",
    "      elif reg_type == \"lasso\" and reg_coef > 0:\n",
    "        model = Lasso(alpha=reg_coef, fit_intercept=False, tol=1e-2, max_iter=10000)\n",
    "        model_key = \"lasso\"\n",
    "      else:\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        model_key = \"linearregression\"\n",
    "      \n",
    "      for noise in noise_values:\n",
    "        y = true_regression_fun(ground_truth)(X) + gauss_noise * noise\n",
    "        clf = make_pipeline(PolynomialFeatures(degree), model)\n",
    "        clf.fit(X[:, np.newaxis], y)\n",
    "        l1_norms.append(np.linalg.norm(clf[model_key].coef_, ord=1))\n",
    "        l2_norms.append(np.linalg.norm(clf[model_key].coef_, ord=2))\n",
    "    \n",
    "      fig.add_trace(go.Scatter(x=noise_values,\n",
    "                               y=l1_norms,\n",
    "                               line_width=3,\n",
    "                               marker_color=DEFAULT_PLOTLY_COLORS[i],\n",
    "                               name=f\"${reg_type.title()}\\ \\lambda={reg_coef}$\",\n",
    "                               legendgroup=f\"${reg_type.title()}\\ \\lambda={reg_coef}$\"),\n",
    "                    row=1,\n",
    "                    col=1)\n",
    "      fig.add_trace(go.Scatter(x=noise_values,\n",
    "                               y=l2_norms,\n",
    "                               line_width=3,\n",
    "                               marker_color=DEFAULT_PLOTLY_COLORS[i],\n",
    "                               name=f\"${reg_type.title()}\\ \\lambda={reg_coef}$\",\n",
    "                               legendgroup=f\"${reg_type.title()}\\ \\lambda={reg_coef}$\",\n",
    "                               showlegend=False),\n",
    "                    row=1,\n",
    "                    col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=go.layout.Margin(\n",
    "            l=0,  #left margin\n",
    "            r=0,  #right margin\n",
    "            b=0,  #bottom margin\n",
    "            t=10,  #top margin\n",
    "        ),\n",
    "        xaxis1_title=\"Noise level\",\n",
    "        yaxis1_title=\"$\\ell_1\\ norm$\",\n",
    "        xaxis2_title=\"Noise level\",\n",
    "        yaxis2_title=\"$\\ell_2\\ norm$\",\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "_ = interact(\n",
    "    norm_increase_with_noise,\n",
    "    n_samples=ipywidgets.IntSlider(value=20,\n",
    "                                   min=10,\n",
    "                                   max=100,\n",
    "                                   step=10,\n",
    "                                   description='Number of samples:',\n",
    "                                   style={'description_width': 'initial'},\n",
    "                                   continuous_update=False),\n",
    "    degree=ipywidgets.IntSlider(value=5,\n",
    "                                min=1,\n",
    "                                max=10,\n",
    "                                step=1,\n",
    "                                description='Polynomial Degree:',\n",
    "                                style={'description_width': 'initial'},\n",
    "                                continuous_update=False),\n",
    "    reg_type=ipywidgets.Dropdown(options=[\"lasso\", \"ridge\"],\n",
    "                                 value=\"ridge\",\n",
    "                                 description='Regularization type:',\n",
    "                                 disabled=False,\n",
    "                                 style={'description_width': 'initial'},\n",
    "                                 continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
